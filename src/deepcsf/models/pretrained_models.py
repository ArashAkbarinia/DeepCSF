"""

"""

import os
import sys
import numpy as np

import torch
import torch.nn as nn

from torchvision.models import segmentation
import torchvision.transforms.functional as torchvis_fun
import clip

from . import model_utils
from . import vqvae
from .taskonomy import taskonomy_network


class LayerActivation(nn.Module):
    def __init__(self, model, layer_name, conv_bn_relu='relu'):
        super(LayerActivation, self).__init__()

        # FIXME: only for resnet at this point
        self.relu = None
        self.sub_layer = None
        self.sub_conv = None

        whole_layers = ['layer%d' % e for e in range(1, 6)]
        if layer_name in whole_layers:
            print('Activation for the whole %s' % layer_name)
            last_areas = [4, 5, 6, 7, 8]
            # FIXME at somepoint go to 0 indexing to avoid all this mess
            lind = last_areas[int(layer_name[-1]) - 1]
            self.features = nn.Sequential(*list(model.children())[:lind])
        elif layer_name == 'fc':
            self.features = model
        elif layer_name == 'avgpool':
            self.features = nn.Sequential(*list(model.children()))
        else:
            if conv_bn_relu == 'relu':
                self.relu = nn.ReLU(inplace=True)
                which_fun = model_utils.resnet_bn_layer
            elif conv_bn_relu == 'bn':
                which_fun = model_utils.resnet_bn_layer
            else:
                which_fun = model_utils.resnet_conv_layer

            name_split = layer_name.split('.')
            sub_layer = None
            sub_conv = None
            # -3 because the features were autogenerated and start from 4
            area_num = int(name_split[0]) - 3
            layer_num = int(name_split[1])
            conv_num = int(name_split[2][-1])
            last_areas = [1, 4, 5, 6, 7]
            last_area = last_areas[area_num]
            if area_num > 0:
                layerx = list(model.children())[last_areas[area_num]]
                sub_layer, sub_conv = which_fun(layerx, layer_num, conv_num)
            self.features = nn.Sequential(*list(model.children())[:last_area])
            self.sub_layer = sub_layer
            self.sub_conv = sub_conv

    def forward(self, x):
        x = self.features(x)
        if self.sub_layer is not None:
            x = self.sub_layer(x)
        if self.sub_conv is not None:
            x = self.sub_conv(x)
        if self.relu is not None:
            x = self.relu(x)
        return x


def generic_features_size(model, target_size):
    img = np.random.randint(0, 256, (target_size, target_size, 3)).astype('float32') / 255
    img = torchvis_fun.to_tensor(img).unsqueeze(0)
    with torch.no_grad():
        out = model(img)
    return np.prod(out[0].shape)


def clip_features(model, network_name, layer):
    if layer == 'encoder':
        features = model
        if 'B32' in network_name or 'B16' in network_name or 'RN101' in network_name:
            org_classes = 512
        elif 'L14' in network_name or 'RN50x16' in network_name:
            org_classes = 768
        elif 'RN50x4' in network_name:
            org_classes = 640
        else:
            org_classes = 1024
    elif network_name.replace('clip_', '') in ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64']:
        if layer == 'area0':
            layer = 8
            org_classes = 200704
        elif layer == 'area1':
            layer = 9
            org_classes = 802816
        elif layer == 'area2':
            layer = 10
            org_classes = 401408
        elif layer == 'area3':
            layer = 11
            org_classes = 200704
        elif layer == 'area4':
            layer = 12
            org_classes = 100352
        features = nn.Sequential(*list(model.children())[:layer])
    return features, org_classes


def resnet_features(model, network_name, layer, target_size):
    if type(layer) is str:
        if layer == 'area0':
            layer = 4
        elif layer == 'area1':
            layer = 5
        elif layer == 'area2':
            layer = 6
        elif layer == 'area3':
            layer = 7
        elif layer == 'area4':
            layer = 8
        elif layer == 'encoder':
            if 'taskonomy_' in network_name:
                layer = len(list(model.children()))
        else:
            sys.exit('Unsupported layer %s' % layer)
    features = nn.Sequential(*list(model.children())[:layer])
    org_classes = generic_features_size(features, target_size)
    return features, org_classes


def get_pretrained_model(network_name, transfer_weights):
    if 'clip' in network_name:
        if 'B32' in network_name:
            clip_version = 'ViT-B/32'
        elif 'B16' in network_name:
            clip_version = 'ViT-B/16'
        elif 'L14' in network_name:
            clip_version = 'ViT-L/14'
        else:
            clip_version = network_name.replace('clip_', '')
        model, _ = clip.load(clip_version)
    elif 'taskonomy_' in network_name:
        # NOTE: always assumed pretrained
        feature_task = network_name.replace('taskonomy_', '')
        model = taskonomy_network.TaskonomyEncoder()
        feature_type_url = taskonomy_network.TASKONOMY_PRETRAINED_URLS[feature_task + '_encoder']
        checkpoint = torch.utils.model_zoo.load_url(feature_type_url, model_dir=None, progress=True)
        model.load_state_dict(checkpoint['state_dict'])
    elif os.path.isfile(transfer_weights[0]):
        # FIXME: cheap hack!
        if 'vqvae' in network_name or 'vqvae' in transfer_weights[0]:
            vqvae_info = torch.load(transfer_weights[0], map_location='cpu')

            backbone = {
                'arch_name': vqvae_info['backbone']['arach'],
                'layer_name': vqvae_info['backbone']['area'],
            }
            # hardcoded to test one type
            hidden = vqvae_info['backbone']['hidden']
            k = vqvae_info['backbone']['k']
            kl = vqvae_info['backbone']['kl']
            model = vqvae.Backbone_VQ_VAE(
                hidden, k=k, kl=kl, num_channels=3, colour_space='rgb2rgb',
                task=None, out_chns=3, cos_distance=False, use_decor_loss=False, backbone=backbone
            )
            model.load_state_dict(vqvae_info['state_dict'])
            print('Loaded the VQVAE model!')
        else:
            model = model_utils.which_network(
                transfer_weights[0], transfer_weights[2],
                num_classes=1000 if 'class' in transfer_weights[2] else 21
            )
    elif '_scratch' in network_name:
        model = model_utils.which_architecture(network_name.replace('_scratch', ''))
    elif 'fcn_' in network_name or 'deeplab' in network_name:
        model = segmentation.__dict__[network_name](pretrained=True)
    else:
        model = model_utils.which_network(transfer_weights[0], 'classification', num_classes=1000)
    return model


def get_backbone(network_name, model):
    if 'clip' in network_name:
        return model.visual
    elif 'vqvae' in network_name:
        return model.backbone_encoder.features
    elif 'fcn_' in network_name or 'deeplab' in network_name:
        return model.backbone
    return model
