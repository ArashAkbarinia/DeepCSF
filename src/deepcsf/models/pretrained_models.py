"""

"""

import os
import sys
import numpy as np

import torch
import torch.nn as nn

from torchvision.models import segmentation
import torchvision.transforms.functional as torchvis_fun
import clip

from . import model_utils
from . import vqvae
from .taskonomy import taskonomy_network


class LayerActivation(nn.Module):
    def __init__(self, model, layer_name, conv_bn_relu='relu'):
        super(LayerActivation, self).__init__()

        # FIXME: only for resnet at this point
        self.relu = None
        self.sub_layer = None
        self.sub_conv = None

        whole_layers = ['layer%d' % e for e in range(1, 6)]
        if layer_name in whole_layers:
            print('Activation for the whole %s' % layer_name)
            last_areas = [4, 5, 6, 7, 8]
            # FIXME at somepoint go to 0 indexing to avoid all this mess
            lind = last_areas[int(layer_name[-1]) - 1]
            self.features = nn.Sequential(*list(model.children())[:lind])
        elif layer_name == 'fc':
            self.features = model
        elif layer_name == 'avgpool':
            self.features = nn.Sequential(*list(model.children()))
        else:
            if conv_bn_relu == 'relu':
                self.relu = nn.ReLU(inplace=True)
                which_fun = model_utils.resnet_bn_layer
            elif conv_bn_relu == 'bn':
                which_fun = model_utils.resnet_bn_layer
            else:
                which_fun = model_utils.resnet_conv_layer

            name_split = layer_name.split('.')
            sub_layer = None
            sub_conv = None
            # -3 because the features were autogenerated and start from 4
            area_num = int(name_split[0]) - 3
            layer_num = int(name_split[1])
            conv_num = int(name_split[2][-1])
            last_areas = [1, 4, 5, 6, 7]
            last_area = last_areas[area_num]
            if area_num > 0:
                layerx = list(model.children())[last_areas[area_num]]
                sub_layer, sub_conv = which_fun(layerx, layer_num, conv_num)
            self.features = nn.Sequential(*list(model.children())[:last_area])
            self.sub_layer = sub_layer
            self.sub_conv = sub_conv

    def forward(self, x):
        x = self.features(x)
        if self.sub_layer is not None:
            x = self.sub_layer(x)
        if self.sub_conv is not None:
            x = self.sub_conv(x)
        if self.relu is not None:
            x = self.relu(x)
        return x


class ViTLayers(nn.Module):
    def __init__(self, parent_model, encoder_layer):
        super().__init__()
        self.parent_model = parent_model
        encoder_layer = encoder_layer + 1
        self.parent_model.encoder.layers = self.parent_model.encoder.layers[:encoder_layer]
        del self.parent_model.heads

    def forward(self, x):
        # Reshape and permute the input tensor
        x = self.parent_model._process_input(x)
        n = x.shape[0]

        # Expand the class token to the full batch
        batch_class_token = self.parent_model.class_token.expand(n, -1, -1)
        x = torch.cat([batch_class_token, x], dim=1)

        x = self.parent_model.encoder(x)

        # Classifier "token" as used by standard language architectures
        x = x[:, 0]

        return x


class ViTClipLayers(nn.Module):
    def __init__(self, parent_model, encoder_layer):
        super().__init__()
        self.parent_model = parent_model
        block = encoder_layer + 1
        self.parent_model.transformer.resblocks = self.parent_model.transformer.resblocks[:block]
        del self.parent_model.proj
        del self.parent_model.ln_post

    def forward(self, x):
        x = self.parent_model.conv1(x)  # shape = [*, width, grid, grid]
        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]
        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]
        x = torch.cat(
            [self.parent_model.class_embedding.to(x.dtype) + torch.zeros(
                x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device),
             x], dim=1
        )  # shape = [*, grid ** 2 + 1, width]
        x = x + self.parent_model.positional_embedding.to(x.dtype)
        x = self.parent_model.ln_pre(x)

        x = x.permute(1, 0, 2)  # NLD -> LND
        x = self.parent_model.transformer(x)
        x = x.permute(1, 0, 2)  # LND -> NLD

        x = x[:, 0, :]

        return x


def vit_features(model, layer, target_size):
    encoder_layer = int(layer.replace('encoder', ''))
    features = ViTLayers(model, encoder_layer)
    org_classes = generic_features_size(features, target_size)
    return features, org_classes


def vgg_features(model, layer, target_size):
    if 'feature' in layer:
        layer = int(layer.replace('feature', '')) + 1
        features = nn.Sequential(*list(model.features.children())[:layer])
    elif 'classifier' in layer:
        layer = int(layer.replace('classifier', '')) + 1
        features = nn.Sequential(
            model.features, model.avgpool, nn.Flatten(1), *list(model.classifier.children())[:layer]
        )
    else:
        sys.exit('Unsupported layer %s' % layer)
    org_classes = generic_features_size(features, target_size)
    return features, org_classes


def generic_features_size(model, target_size, dtype=None):
    img = np.random.randint(0, 256, (target_size, target_size, 3)).astype('float32') / 255
    img = torchvis_fun.to_tensor(img).unsqueeze(0)
    if dtype is not None:
        img = img.cuda()
        img = img.type(dtype)
    model.eval()
    with torch.no_grad():
        out = model(img)
    return np.prod(out[0].shape)


def clip_features(model, network_name, layer, target_size):
    if layer == 'encoder':
        features = model
        if 'B32' in network_name or 'B16' in network_name or 'RN101' in network_name:
            org_classes = 512
        elif 'L14' in network_name or 'RN50x16' in network_name:
            org_classes = 768
        elif 'RN50x4' in network_name:
            org_classes = 640
        else:
            org_classes = 1024
    elif network_name.replace('clip_', '') in ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64']:
        if layer == 'area0':
            layer = 10
            org_classes = 200704
        elif layer == 'area1':
            layer = 11
            org_classes = 802816
        elif layer == 'area2':
            layer = 12
            org_classes = 401408
        elif layer == 'area3':
            layer = 13
            org_classes = 200704
        elif layer == 'area4':
            layer = 14
            org_classes = 100352
        features = nn.Sequential(*list(model.children())[:layer])
    else:
        block_layer = int(layer.replace('block', ''))
        features = ViTClipLayers(model, block_layer)
        org_classes = generic_features_size(features, target_size, model.conv1.weight.dtype)
    return features, org_classes


def regnet_features(model, layer, target_size):
    if 'stem' in layer:
        features = model.stem
    elif 'block' in layer:
        if layer == 'block1':
            layer = 1
        elif layer == 'block2':
            layer = 2
        elif layer == 'block3':
            layer = 3
        elif layer == 'block4':
            layer = 4
        features = nn.Sequential(model.stem, *list(model.trunk_output.children())[:layer])
    else:
        sys.exit('Unsupported layer %s' % layer)
    org_classes = generic_features_size(features, target_size)
    return features, org_classes


def resnet_features(model, network_name, layer, target_size):
    if layer == 'area0':
        layer = 4
    elif layer == 'area1':
        layer = 5
    elif layer == 'area2':
        layer = 6
    elif layer == 'area3':
        layer = 7
    elif layer == 'area4':
        layer = 8
    elif layer == 'encoder':
        if 'taskonomy_' in network_name:
            layer = len(list(model.children()))
    else:
        sys.exit('Unsupported layer %s' % layer)
    features = nn.Sequential(*list(model.children())[:layer])
    org_classes = generic_features_size(features, target_size)
    return features, org_classes


def get_pretrained_model(network_name, transfer_weights):
    if 'clip' in network_name:
        if 'B32' in network_name:
            clip_version = 'ViT-B/32'
        elif 'B16' in network_name:
            clip_version = 'ViT-B/16'
        elif 'L14' in network_name:
            clip_version = 'ViT-L/14'
        else:
            clip_version = network_name.replace('clip_', '')
        model, _ = clip.load(clip_version)
    elif 'taskonomy_' in network_name:
        # NOTE: always assumed pretrained
        feature_task = network_name.replace('taskonomy_', '')
        model = taskonomy_network.TaskonomyEncoder()
        feature_type_url = taskonomy_network.TASKONOMY_PRETRAINED_URLS[feature_task + '_encoder']
        checkpoint = torch.utils.model_zoo.load_url(feature_type_url, model_dir=None, progress=True)
        model.load_state_dict(checkpoint['state_dict'])
    elif os.path.isfile(transfer_weights[0]):
        # FIXME: cheap hack!
        if 'vqvae' in network_name or 'vqvae' in transfer_weights[0]:
            vqvae_info = torch.load(transfer_weights[0], map_location='cpu')

            backbone = {
                'arch_name': vqvae_info['backbone']['arach'],
                'layer_name': vqvae_info['backbone']['area'],
            }
            # hardcoded to test one type
            hidden = vqvae_info['backbone']['hidden']
            k = vqvae_info['backbone']['k']
            kl = vqvae_info['backbone']['kl']
            model = vqvae.Backbone_VQ_VAE(
                hidden, k=k, kl=kl, num_channels=3, colour_space='rgb2rgb',
                task=None, out_chns=3, cos_distance=False, use_decor_loss=False, backbone=backbone
            )
            model.load_state_dict(vqvae_info['state_dict'])
            print('Loaded the VQVAE model!')
        else:
            model = model_utils.which_network(
                transfer_weights[0], transfer_weights[2],
                num_classes=1000 if 'class' in transfer_weights[2] else 21
            )
    elif '_scratch' in network_name:
        model = model_utils.which_architecture(network_name.replace('_scratch', ''))
    elif 'fcn_' in network_name or 'deeplab' in network_name:
        model = segmentation.__dict__[network_name](pretrained=True)
    else:
        model = model_utils.which_network(transfer_weights[0], 'classification', num_classes=1000)
    return model


def get_backbone(network_name, model):
    if 'clip' in network_name:
        return model.visual
    elif 'vqvae' in network_name:
        return model.backbone_encoder.features
    elif 'fcn_' in network_name or 'deeplab' in network_name:
        return model.backbone
    return model
